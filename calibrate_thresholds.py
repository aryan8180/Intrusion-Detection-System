#!/usr/bin/env python3
"""
calibrate_thresholds.py
- Loads models/best_model.joblib and data/captured_flows.csv
- Computes IsolationForest decision_function scores on the dataset
- Computes percentile thresholds (1st and 5th by default)
- Saves thresholds to config/thresholds.json
Usage:
    python calibrate_thresholds.py
"""

import os
import json
import numpy as np
import pandas as pd
import joblib

MODEL_FILE = os.path.join("models", "best_model.joblib")
DATA_FILE = os.path.join("data", "captured_flows.csv")
OUT_FILE = os.path.join("config", "thresholds.json")

# Percentiles to use
P_HIGH = 1.0   # 1st percentile -> HIGH severity
P_MED  = 5.0   # 5th percentile -> MEDIUM severity

if not os.path.exists(MODEL_FILE):
    raise SystemExit("Model not found: models/best_model.joblib. Train a model first.")

if not os.path.exists(DATA_FILE):
    raise SystemExit("Data file not found: data/captured_flows.csv. Capture some traffic first.")

# Load model and scaler
bundle = joblib.load(MODEL_FILE)
model = bundle.get("model")
scaler = bundle.get("scaler")
mtype = bundle.get("type","isolation")

# We only calibrate for unsupervised (isolation). If supervised, skip.
if mtype != "isolation":
    print("Model type is supervised ('{}'). Calibration for percentiles is not required but you may still compute decision scores.".format(mtype))

# Load CSV and prepare features (same features used in training)
df = pd.read_csv(DATA_FILE)
feats = ["pkt_count","byte_count","duration","avg_pkt_size","pkt_iat_mean","syn_count","ack_count","rst_count"]
missing = [f for f in feats if f not in df.columns]
if missing:
    raise SystemExit(f"Missing features in CSV: {missing}. Ensure capture_live.py wrote correct headers and data.")

X = df[feats].astype(float).values
# scale
if scaler is not None:
    Xs = scaler.transform(X)
else:
    # no scaler present, scale with simple mean/var to avoid crash
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler().fit(X)
    Xs = sc.transform(X)

# compute decision scores (IsolationForest) or predicted probabilities for supervised
if hasattr(model, "decision_function"):
    scores = model.decision_function(Xs)
else:
    # fallback: if supervised classifier, use predict_proba or predict
    if hasattr(model, "predict_proba"):
        # we take probability of the positive (malicious) class as "score"
        probs = model.predict_proba(Xs)[:,1]
        # invert so that more anomalous -> lower score (to keep same interface)
        scores = probs * -1.0
    else:
        preds = model.predict(Xs)
        # discrete predictions -> convert to -1 (anom) / +1 (normal) style
        scores = np.array([ -1.0 if p==1 else 0.0 for p in preds ])

# compute percentiles
p_high = float(np.percentile(scores, P_HIGH))
p_med  = float(np.percentile(scores, P_MED))

# Save to config JSON
os.makedirs("config", exist_ok=True)
payload = {
    "model_type": mtype,
    "percentile_high": P_HIGH,
    "percentile_med": P_MED,
    "threshold_high": p_high,
    "threshold_medium": p_med,
    "notes": "Generated by calibrate_thresholds.py"
}
with open(OUT_FILE, "w") as fw:
    json.dump(payload, fw, indent=2)

print("Calibration complete. Thresholds written to", OUT_FILE)
print("HIGH threshold ({}th pct): {:.6f}".format(P_HIGH, p_high))
print("MED  threshold ({}th pct): {:.6f}".format(P_MED, p_med))
